{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Running Python / IPython / PySpark on the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note:_ I haven't included the results of running cells in this notebook because I no longer have access to the test cluster on which these exercises were run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've created a cluster for you on AWS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... _tour of machines on AWS and of Cloudera Manager_ ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything you've done so far on your machine, you can do remotely on our cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 3.1.1  [Optional, only if you use SSH / Linux regularly]  You can open an SSH connection to our cluster and run `python` and `ipython` terminals directly.  Open an SSH connection (e.g., using PuTTY) to our main machine like this:**\n",
    "```\n",
    "ssh client@52.29.121.97 -p 443    [port 443, which is open in your corporate firewall]\n",
    "[password: client12345]\n",
    "```\n",
    "**Now type `python` and evaluate `2+2`.  Hit Ctrl-D to exit.  Now type `ipython` and evaluate `2+2`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 3.1.2 [Optional, only if you use SSH / Linux regularly]  I've placed a file `pyscript.py` at in the home folder of the `client` user, with the following contents:**\n",
    "```\n",
    "import sys\n",
    "if len(sys.argv) >= 2:\n",
    "    print(\"Hello, {0}\".format(sys.argv[1]))\n",
    "else:\n",
    "    print(\"Who are you?\")\n",
    "```\n",
    "**Give this script a go by typing `python pyscript.py <yourname>` and `python pyscript.py`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During all of our training, we've used IPython notebooks instead of the python terminals and python scripts.  Many people think that this is the easiest and cleanest work to run the exploratory and interactive workloads that are typical of data analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like some of you connected to the IPython Notebook server on my laptop yesterday, you can create, edit and run code in IPython Notebooks in any other server.  I've set up an IPython Notebook on our cluster (with `pyspark` already configured) at the following address:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>[http://52.29.121.97](http://52.29.121.97)</center>  \n",
    "<center>This server is **the most insecure server in the planet** (only half-joking here!).  **DO NOT** upload sensitive data in here.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[_Note to Patrick to start up that server: In a terminal, `ssh ipython@52.29.121.97` and type `IPYTHON_OPTS=notebook pyspark`.  In another terminal, `ssh ubuntu@52.29.121.97` and type `sudo ssh ubuntu@localhost -L 0.0.0.0:80:localhost:8889`_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IPython Notebook is password-protected to at least hold off hackers for a few minutes.  The password is:\n",
    "\n",
    "<center>**`client12345`**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 3.1.3 Log into the IPython Notebook server on our cluster and create your own notebook.  Play around with the notebook for a few minutes to make sure everything works as you expect:**\n",
    "1. **Import `numpy`, build a few arrays and perform some vector operations on them.**\n",
    "2. **Import `matplotlib` and plot sin(x) from 0 to 2 $\\pi$.**\n",
    "3. **Import `pandas` and create a DataFrame with the GDP per capita of BE and NL.  Calculate the mean GDP per capita of BE and NL in the year 2003**\n",
    "4. **Import `scikit-learn` and run the example of a linear regression on simulated data from yesterday morning**  _Note: this should also test out SciPy_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for something more interesting: Spark on the cluster!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 3.1.4: I've loaded all the names dataset into a single file `allnames.txt`, now with an extra year column.  The first few lines read:**\n",
    "```\n",
    "Mary,F,7065,1880\n",
    "Anna,F,2604,1880\n",
    "Emma,F,2003,1880\n",
    "Elizabeth,F,1939,1880\n",
    "```\n",
    "**Try to read it into an RDD using `sc.textFile(\"file:///home/ipython/allnames.txt\")`, then take the first 5 items.  What error message do you get?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key portion of the error message is this:\n",
    "```\n",
    "...node6.test-hadoop-internal.dataminded.be): java.io.FileNotFoundException: File file:/home/ipython/allnames.txt does not exist\n",
    "```\n",
    "The file `allnames.txt` is in the machine where we're running the driver code and IPython notebook (`manager.test-hadoop-internal.dataminded.be`), but the worker node (`node6`) is trying to read the file `allnames.txt` on its **local** filesystem.  Of course, the file _isn't_ there!\n",
    "\n",
    "Welcome to distributed computing :-D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a cluster environment, we need a **distributed file system** where:\n",
    "* All the disk space on the cluster looks like one giant disk\n",
    "* Any machine can access any data on the file system, regardless of where that data is located.\n",
    "* Any disk failures are handled gracefully and transparently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Hadoop cluster, these requirements are met by the **Hadoop Distributed File System**, or **HDFS**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't talk much about the internals of HDFS, but we do want to know how to use it.  The Cloudera distribution of Hadoop includes a nice user interface to the cluster called **Hue** (Hadoop User Experience).  With it, you can explore the files in HDFS, and easily upload and download files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log into Hue as follows:\n",
    "<center>[http://52.29.121.97:443](http://52.29.121.97:443) (MUST BE http, NOT https!)</center>\n",
    "\n",
    "<center>Username: **`ipython`**</center>\n",
    "<center>Password: **`ipython12345`**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note to Patrick: To make this happen, edit /etc/ssh/sshd_config to remove Port 443 and restart the ssh service.  Then close the tunnel on the ubuntu machine and run the following tunnel instead: sudo ssh ubuntu@localhost -L 0.0.0.0:80:localhost:8889 -L 0.0.0.0:443:manager:8888 _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Quick tour of Hue and of HDFS_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've uploaded the `allnames.txt` file into HDFS, under `hdfs:///user/ipython/names/allnames.txt`.  We can use that as the parameter for `sc.textFile()`.  In fact, since the IPython notebook server is already running as the `ipython` user, if you just say this:\n",
    "```\n",
    "rdd = sc.textFile('names/allnames.txt')\n",
    "```\n",
    "then Spark already knows to look in HDFS under `/user/ipython`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 3.1.5 Your first Spark job on the cluster!**\n",
    "1. **Use Spark Core to load this file into an RDD and count the number of births between 1950 and 2000 with baby names starting in 'M'**\n",
    "2. **Use Spark SQL and matplotlib to plot the number of boy and girl births for each year in the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you're running your jobs, I'll show you the Cloudera Manager UI so that you can see the amount of CPU and disk I/O that you're exerting on the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice two things:\n",
    "* Even though you're now running your jobs over 8 nodes with 16GB of RAM and 2 cores each, and you're reading the data from a distributed file system, the Spark code is virtually identical to what you used on your laptop.\n",
    "* The overhead to distribute computation is now quite substantial.  The `allnames.txt` file is only 30MB in size.  **30MB is not Big Data**.  The overhead introduced by distributing the computation far outweighs the actual computational and network costs at this point.\n",
    "\n",
    "**Lesson:** for anything below `100s MB to 1 GB`, use your laptop and the \"small- to medium-scale\" packages from early in the training (NumPy, SciPy, Pandas, Matplotlib, Scikit-learn)!  Only use the big data packages when your data actually is big.  A few exceptions:\n",
    "1. Use Spark if the data is already on your cluster and you either can't move it out or it will take a long time to move it out.\n",
    "2. Prepare your Spark codes on small amounts of data on your laptop to debug the easy problems.  Once your code works for small datasets, then (and only then) apply it to your Big Data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 3.1.6 Saving your results to HDFS.  Say you want to get the complete list of unique names starting with 'M' from 1950 to 2000.  The list is quite large.**\n",
    "1. **Write a Spark Core snippet to calculate that list in an RDD, but don't call `collect()` on it**\n",
    "2. **Instead of using `collect()` to bring it all back to the driver node, you can use `saveAsTextFile('<yourname>.txt')` to write out the RDD to HDFS.  Do that, then use `sc.textFile()` and `take` to verify that the first few lines of the file look ok.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 3.2.7 Go find your file in HDFS.  Does anything look odd??**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's going on here??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/saveAsTextFileMakesAFolder1.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"file\" `patrick.txt` is actually a folder in HDFS!!  And what's in there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/saveAsTextFileMakesAFolder2.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple files??  Welcome to distributed computing :-D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's happening is that Spark is trying to avoid sending all the data back to driver node to write it out into a single file.  Instead, Spark creates a folder called `\"patrick.txt\"` and **each worker node** creates a file with its share of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Hadoop clusters, this is a very common pattern used to avoid concentrating all the data in one node.  So if you ever specify a folder in the argument of `sc.textFile()`, Spark will automatically read every file in that folder and (conceptually) treat all the lines of all the files as one single RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other thing looks strange: why did Spark only use two worker nodes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 3.1.8 When you load up `allnames.txt`, into how many partitions does Spark split the data?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 3.1.9 You can use `repartition(N)` to force Spark to redistribute the data evenly across a different number of partitions.  With 30 MB, it makes little sense to use too many partitions.  But just for experimenting, repartition the data into 16 partitions, then run your code to count the number of births from 1950 to 2000 whose names start with 'M'.  Is is faster?  Slower?  The same?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Hint_: To time a piece of code, you can use the `datetime` module as follows:\n",
    "```\n",
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "... YOUR CODE HERE ...\n",
    "stop = datetime.now()\n",
    "elapsed_s = (stop - start).total_seconds()\n",
    "print(\"Took {0:.2f} s\".format(elapsed_s))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repartitioning is usually used in two scenarios:\n",
    "* You start with lots of small files (i.e., lots of small partitions) and want to coalesce the data into larger partitions to reduce the Spark overhead per partition.\n",
    "* Your per-item computation is very expensive, and you want to distribute it among as many machines as possible\n",
    "\n",
    "Repartitioning is not free: data has to be move across machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hive is a SQL-on-Hadoop server.  You can expose files in HDFS as SQL-like tables, then issue SQL-like queries against them.  This isn't a Hive training course, but I do want you to be able to access data in Hive via Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_tour of Hive in Hue_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 3.1.10 Go to the Hive Editor in Hue and issue the following query to expose the names data as a table in Hive. Replace YOURNAME with your name so that we don't clash with each other:**\n",
    "```\n",
    "CREATE EXTERNAL TABLE hive_names_YOURNAME (\n",
    "  name STRING,\n",
    "  sex STRING,\n",
    "  births INT,\n",
    "  year INT\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    "LOCATION 'hdfs:///user/ipython/names';\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `CREATE TABLE` statement doesn't actually load the data.  It just tells Hive that the data exists in some location and has a CSV format with 4 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 3.1.11 If you can write SQL, you can write Hive.  Run a Hive query against your table to count the number of births between 1950 and 2000 whose names start with 'M'**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of your data in the production cluster is already loaded into Hive.  If you can connect Spark to Hive, you can use your production data without too much effort.  Here's how to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import HiveContext, Row\n",
    "hiveCtx = HiveContext(sc)\n",
    "\n",
    "tableRDD = hiveCtx.table(\"hive_names_patrick\")  # Must run this on the cluster, not on your laptop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "That's it!  Now you can use the table from Hive like any other `SchemaRDD`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 3.1.12 Use Spark Core to get all distinct names starting with 'M' from the year 1880 from the Hive table \"hive_names_YOURNAME\"**  \n",
    "_WARNING_: Spark 1.3 onwards changed the type of Spark SQL objects from `SchemaRDD` to `DataFrame`, which has a different API.  To use the API that works in Spark 1.2, you have to get the `rdd` property of the table, e.g.:\n",
    "```\n",
    "tableRDD.rdd.filter(lambda row: row.year == 2000).count()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also just issue SQL commands that run against your Hive tables.  If you register `SchemaRDD` in the Hive context, you can even do things like join your Hive tables against temporary Spark SQL tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hiveCtx.sql(\n",
    "    \"SELECT year, SUM(births) \"\n",
    "    \"FROM hive_names_patrick GROUP BY year ORDER BY year ASC\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 = \"hello\\nworld\"\n",
    "s2 = \"\"\"hello\n",
    "world\"\"\"\n",
    "s3 = \"hello hello hello hello\"\n",
    "s4 = (\"hello hello \"\n",
    "      \"hello hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any DDL statements that you execute will create Hive-visible tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 3.1.13 Using `\"CREATE TABLE hive_births_YOURNAME AS SELECT ...\"`, create a table in Hive with the number of births per year.  Look up this table in Hue and verify that the data is accessible from outside your Spark session now**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(!) Ex 3.1.14  To get a good feel for working on the cluster and comparing cluster-code to laptop-code, upload the MLLib IPython Notebook and adapt it so that every machine learning example runs on the cluster instead of on your (or my) laptop.  You should find that there's not too much work to be done!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
