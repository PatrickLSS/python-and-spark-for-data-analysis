{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3: Python and Apache Spark on a Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yesterday, we talked generally about distributed computing, and learned about Spark Core and Spark SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributed computing is quite different from working on your laptop:\n",
    "* Each node only holds **part of the data** and moving data around is expensive\n",
    "* Code is tiny compared to data, so we **send code to the data** instead of downloading data to your machine.\n",
    "* **Failure is ubiquitous**: nodes go down, disks die, power goes off, AC fails, ...  all the time.  You need to store data redundantly and be prepared to detect failed partial calculations and rerun them elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Spark` is a framework for writing distributed computations that handles most of the complications of distributed computing transparently.  The devil's bargain is that you need to **write code in a very functional style**, in order to expose the structure and parallelism of your calculations, as well as minimize data transfer and intermediate results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main abstraction is an RDD, or Resilient Distributed Dataset.  Like a NumPy array or Python lists, but the data is split up among all the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/pat/Work/2015/PythonTraining/4DayTrainingOpenSource'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Record current working directory for later use\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A few ways to make an initial RDD\n",
    "\n",
    "# Items in a Python list (also NumPy array, Pandas series, ...)\n",
    "rdd1 = sc.parallelize([1,2,3,4,5])\n",
    "\n",
    "# Lines in a file\n",
    "rdd2 = sc.textFile(\"file://\" + cwd + \"/names/yob1880.txt\")\n",
    "\n",
    "# Files in a folder\n",
    "# Items are (filename, contents) tuples\n",
    "rdd3 = sc.wholeTextFiles(\"file://\" + cwd + \"/names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One RDD can be transformed into another with a **transformation**.  The main ones:\n",
    "* `map()`\n",
    "* `flatMap()`\n",
    "* `filter()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'patrick']\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(['Hello world', 'My name is Patrick'])\n",
    "print(\n",
    "    rdd\n",
    "    .flatMap(lambda sentence: sentence.split(' '))\n",
    "    .map(lambda word: word.lower())\n",
    "    .filter(lambda word: len(word) >= 5)\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some transformations produce one RDD from two or more input RDDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 2, 3, 4]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aRDD = sc.parallelize([1,2,3])\n",
    "bRDD = sc.parallelize([2,3,4])\n",
    "\n",
    "aRDD.union(bRDD).collect()   # Also intersection, subtract and cartesian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations are applied **lazily**: Spark will delay computations as long as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Actions** demand an immediate result.  The most common are:\n",
    "* `collect()`\n",
    "* `first()`\n",
    "* `count()`\n",
    "* `take()`\n",
    "* `reduce()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3628800"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10! in Spark\n",
    "sc.parallelize(range(1, 10+1)).reduce(lambda x, y: x * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pair RDDs**, where each item is a `(key, value)` pair, have lots more transformations and actions that work on the keys:\n",
    "* `groupByKey()`\n",
    "* `reduceByKey()`\n",
    "* `countByKey()`\n",
    "* `collectAsMap()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 'new' appears 1 time(s)\n",
      "Word 'says' appears 1 time(s)\n",
      "Word 'hello' appears 4 time(s)\n",
      "Word 'york' appears 2 time(s)\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(['Hello hello', \n",
    "                      'Hello New York', \n",
    "                      'York says hello'])\n",
    "result = (\n",
    "    rdd\n",
    "    .flatMap(lambda sentence: sentence.split(' '))\n",
    "    .map(lambda word: (word.lower(), 1))\n",
    "    .countByKey()    # Could also be .reduceByKey(lambda x, y: x + y).collectAsMap()\n",
    ")\n",
    "for word, count in result.items():\n",
    "    print(\"Word '{0}' appears {1} time(s)\".format(word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pair RDDs can also be **joined** by key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2000, (21205, 21480)), (1913, (4220, 4049)), (1950, (5462, 5996))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "be_gdp_per_capita = sc.parallelize(\n",
    "    [(1913, 4220), (1950, 5462), (2000, 21205)])\n",
    "nl_gdp_per_capita = sc.parallelize(\n",
    "    [(1913, 4049), (1950, 5996), (2000, 21480)])\n",
    "be_gdp_per_capita.join(nl_gdp_per_capita).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1913, -4.052132701421796),\n",
       " (1950, 9.77663859392164),\n",
       " (2000, 1.2968639471822696)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(be_gdp_per_capita.join(nl_gdp_per_capita)\n",
    " .mapValues(lambda (be, nl): (float(nl) / float(be) - 1.0)*100)\n",
    " .sortByKey()\n",
    " .collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many data manipulations, the Spark Core API is a bit too low-level.  Spark SQL is often more convenient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(year=1913, mean=4134.5),\n",
       " Row(year=1950, mean=5729.0),\n",
       " Row(year=2003, mean=21342.5)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext, Row\n",
    "sqlCtx = SQLContext(sc)\n",
    "\n",
    "rdd = sc.parallelize([\n",
    "        Row(country='BE', year=1913, gdp_per_capita=4220),\n",
    "        Row(country='BE', year=1950, gdp_per_capita=5462),\n",
    "        Row(country='BE', year=2003, gdp_per_capita=21205),\n",
    "        Row(country='NL', year=1913, gdp_per_capita=4049),\n",
    "        Row(country='NL', year=1950, gdp_per_capita=5996),\n",
    "        Row(country='NL', year=2003, gdp_per_capita=21480),\n",
    "    ])\n",
    "schemaRDD = sqlCtx.inferSchema(rdd)\n",
    "schemaRDD.registerTempTable(\"gdp\")\n",
    "\n",
    "resultRDD = sqlCtx.sql(\"\"\"\n",
    "    SELECT\n",
    "       year,\n",
    "       AVG(gdp_per_capita) as mean\n",
    "    FROM gdp\n",
    "    GROUP BY year\n",
    "    ORDER BY year ASC\n",
    "    \"\"\"\n",
    ")\n",
    "resultRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Year 1913 has GDP/capita of 4134.5',\n",
       " 'Year 1950 has GDP/capita of 5729.0',\n",
       " 'Year 2003 has GDP/capita of 21342.5']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultRDD.map(lambda row: \"Year {0} has GDP/capita of {1}\"\n",
    "             .format(row.year, row.mean)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also saw contrasted `scikit-learn` with `MLLib`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we'll move to a cluster running Cloudera Hadoop on AWS.  By the end of today, you should be able to:\n",
    "* Run python scripts on the cluster from a shell and from ipython notebooks\n",
    "* Use Spark to read from and write to HDFS\n",
    "* Use SparkSQL to read data from and write data to Hive\n",
    "* Understand how YARN works\n",
    "* Submit spark jobs on the cluster\n",
    "* Use Spark, SparkSQL and Spark MLlib to run algorithms on large-scale data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember:**  The afternoon will be a free-form session for you to play with Spark on the cluster with your own data, to do an analysis that *you* care about.  I'll be available to help you out and answer questions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
