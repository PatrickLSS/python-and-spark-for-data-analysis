{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note:_ I haven't included the results of running cells in this notebook because I no longer have access to the test cluster on which these exercises were run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 3.1.3 Log into the IPython Notebook server on our cluster and create your own notebook.  Play around with the notebook for a few minutes to make sure everything works as you expect:**\n",
    "1. **Import `numpy`, build a few arrays and perform some vector operations on them.**\n",
    "2. **Import `matplotlib` and plot sin(x) from 0 to 2 $\\pi$.**\n",
    "3. **Import `pandas` and create a DataFrame with the GDP per capita of BE and NL.  Calculate the mean GDP per capita of BE and NL in the year 2003**\n",
    "4. **Import `scikit-learn` and run the example of a linear regression on simulated data from yesterday morning**  _Note: this should also test out SciPy_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "A = np.array([1,2,3])\n",
    "B = np.array([4,5,6])\n",
    "C = A + 3*B\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "x = np.linspace(0.0, 2*math.pi, 100)\n",
    "y = np.sin(x)\n",
    "plt.plot(x, y, 'r-')\n",
    "plt.xlabel('Angle')\n",
    "plt.ylabel('$\\sin(x)$')\n",
    "plt.title('Back to high-school')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "data = {\n",
    "    'country': ['BE', 'BE', 'BE', 'NL', 'NL', 'NL'],\n",
    "    'year': [1913, 1950, 2003, 1913, 1950, 2003],\n",
    "    'gdp_per_capita': [4220, 5462, 21205, 4049, 5996, 21480]\n",
    "}\n",
    "frame = DataFrame(data)\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frame[frame.year == 2003].gdp_per_capita.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_data_sample(N=40, rseed=0, m=3, b=-2):\n",
    "    \"\"\"Sample N points on the line y = m*x + b + err, where err is normally distributed\"\"\"\n",
    "    rng = np.random.RandomState(rseed)\n",
    "\n",
    "    x = 10 * rng.rand(N)\n",
    "    y = m * x + b + 1.5*rng.randn(N)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y = linear_data_sample()\n",
    "plt.plot(x, y, 'o');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Least-squares linear Regression, the pedestrian way\n",
    "def model(theta, x):\n",
    "    b, m = theta\n",
    "    return m * x + b\n",
    "\n",
    "def square_deviation(theta, x, y):\n",
    "    return np.sum((model(theta, x) - y) ** 2)\n",
    "\n",
    "from scipy.optimize import fmin\n",
    "theta_guess = [0, 1]\n",
    "theta_fit = fmin(square_deviation, theta_guess, args=(x, y))\n",
    "\n",
    "b_fit, m_fit = theta_fit\n",
    "print 'Actual: m = 3, b = -2'\n",
    "print 'Fit: m = %.2f, b = %.2f' % (m_fit, b_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regr = linear_model.LinearRegression(fit_intercept=True)\n",
    "regr.fit(x.reshape((len(x), 1)), y)   # Scikit-learn expects x to be a column-vector\n",
    "\n",
    "print 'Actual: m = 3, b = -2'\n",
    "print 'Fit: m = %.2f, b = %.2f' % (regr.coef_[0], regr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict y from the data\n",
    "x_new = np.linspace(0, 10, 100)\n",
    "y_new = regr.predict(x_new.reshape((len(x_new), 1)))  # Column vector\n",
    "\n",
    "# plot the results\n",
    "ax = plt.axes()\n",
    "ax.scatter(x, y)\n",
    "ax.plot(x_new, y_new)\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "\n",
    "ax.axis('tight');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 3.2.4: I've loaded all the names dataset into a single file `allnames.txt`, now with an extra year column.  The first few lines read:**\n",
    "```\n",
    "Mary,F,7065,1880\n",
    "Anna,F,2604,1880\n",
    "Emma,F,2003,1880\n",
    "Elizabeth,F,1939,1880\n",
    "```\n",
    "**Try to read it into an RDD using `sc.textFile(\"file:///home/ipython/allnames.txt\")`, then take the first 5 items.  What error message do you get?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile('file:///home/ipython/allnames.txt')\n",
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 3.1.5 Your first Spark job on the cluster!**\n",
    "1. **Use Spark Core to load this file into an RDD and count the number of births between 1950 and 2000 with baby names starting in 'M'**\n",
    "2. **Use Spark SQL and matplotlib to plot the number of boy and girl births for each year in the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile('names/allnames.txt')\n",
    "rdd.cache()\n",
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(rdd\n",
    " .map(lambda line: line.split(','))\n",
    " .map(lambda fields: (fields[0], int(fields[2]), int(fields[3])))\n",
    " .filter(lambda (name, births, year): 1950 <= year <= 2000)\n",
    " .filter(lambda (name, births, year): name.startswith('M'))\n",
    " .map(lambda (name, births, year): births)\n",
    ").sum()    # .sum() is the same as .reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext, Row\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rowsRDD = (rdd\n",
    "           .map(lambda line: line.split(','))\n",
    "           .map(lambda fields: Row(name=fields[0], sex=fields[1], births=int(fields[2]), year=int(fields[3])))\n",
    "           )\n",
    "schemaRDD = sqlCtx.inferSchema(rowsRDD)   # In our cluster, we use Spark 1.5\n",
    "schemaRDD.cache()                         # and inferSchema() will yield a deprecation warning\n",
    "schemaRDD.registerTempTable(\"names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultRDD = sqlCtx.sql(\"\"\"\n",
    "    SELECT\n",
    "        year,\n",
    "        SUM(CASE WHEN sex='M' THEN births ELSE 0 END) AS boys,\n",
    "        SUM(CASE WHEN sex='F' THEN births ELSE 0 END) AS girls\n",
    "    FROM names\n",
    "    GROUP BY year\n",
    "    ORDER BY year ASC\n",
    "\"\"\")\n",
    "result = resultRDD.map(lambda row: (row.year, row.boys, row.girls)).collect()\n",
    "result[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "years = [year for (year, _, _) in result]\n",
    "girls = [girls for (_, girls, _) in result]\n",
    "boys = [boys for (_, _, boys) in result]\n",
    "plt.plot(years, girls, 'r-', label='Girls')\n",
    "plt.plot(years, boys, 'b-', label='Boys')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of births')\n",
    "plt.title('Babies born in US each year, by Gender')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Ex 3.1.6 Saving your results to HDFS.  Say you want to get the complete list of unique names starting with 'M' from 1950 to 2000.  The list is quite large.**\n",
    "1. **Write a Spark Core snippet to calculate that list in an RDD, but don't call `collect()` on it**\n",
    "2. **Instead of using `collect()` to bring it all back to the driver node, you can use `saveAsTextFile('<yourname>.txt')` to write out the RDD to HDFS.  Do that, then use `sc.textFile()` and `take` to verify that the first few lines of the file look ok.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "namesRDD = (rdd\n",
    " .map(lambda line: line.split(','))\n",
    " .map(lambda fields: (fields[0], int(fields[3])))\n",
    " .filter(lambda (name, year): 1950 <= year <= 2000)\n",
    " .filter(lambda (name, year): name.startswith('M'))\n",
    " .map(lambda (name, year): name)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "namesRDD.saveAsTextFile('patrick.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "namesAgainRDD = sc.textFile('patrick.txt')\n",
    "namesAgainRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 3.1.8 When you load up `allnames.txt`, into how many partitions does Spark split the data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# From the horse's mouth\n",
    "sc.textFile('names/allnames.txt').getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 3.1.9 You can use `repartition(N)` to force Spark to redistribute the data evenly across a different number of partitions.  With 30 MB, it makes little sense to use too many partitions.  But just for experimenting, repartition the data into 16 partitions, then run your code to count the number of births from 1950 to 2000 whose names start with 'M'.  Is is faster?  Slower?  The same?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "\n",
    "(sc.textFile('names/allnames.txt')\n",
    " .map(lambda line: line.split(','))\n",
    " .map(lambda fields: (fields[0], int(fields[2]), int(fields[3])))\n",
    " .filter(lambda (name, births, year): 1950 <= year <= 2000)\n",
    " .filter(lambda (name, births, year): name.startswith('M'))\n",
    " .map(lambda (name, births, year): births)\n",
    ").sum()    # .sum() is the same as .reduce(lambda x, y: x + y)\n",
    "\n",
    "stop = datetime.now()\n",
    "elapsed_s = (stop - start).total_seconds()\n",
    "print(\"Without repartition, took {0:.2f} s\".format(elapsed_s))\n",
    "\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "(sc.textFile('names/allnames.txt')\n",
    " .repartition(16)\n",
    " .map(lambda line: line.split(','))\n",
    " .map(lambda fields: (fields[0], int(fields[2]), int(fields[3])))\n",
    " .filter(lambda (name, births, year): 1950 <= year <= 2000)\n",
    " .filter(lambda (name, births, year): name.startswith('M'))\n",
    " .map(lambda (name, births, year): births)\n",
    ").sum()    # .sum() is the same as .reduce(lambda x, y: x + y)\n",
    "\n",
    "stop = datetime.now()\n",
    "elapsed_s = (stop - start).total_seconds()\n",
    "print(\"With repartition, took {0:.2f} s\".format(elapsed_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 3.1.11 If you can write SQL, you can write Hive.  Run a Hive query against your table to count the number of births between 1950 and 2000 whose names start with 'M'**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will work:\n",
    "```\n",
    "SELECT\n",
    "   SUM(births)\n",
    "FROM hive_names_patrick\n",
    "WHERE (year BETWEEN 1950 and 2000) AND (name LIKE 'M%');\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 3.1.12 Use Spark Core to get all distinct names starting with 'M' from the year 1880 from the Hive table \"hive_names_YOURNAME\"**  \n",
    "_WARNING_: Spark 1.3 onwards changed the type of Spark SQL objects from `SchemaRDD` to `DataFrame`, which has a different API.  To use the API that works in Spark 1.2, you have to get the `rdd` property of the table, e.g.:\n",
    "```\n",
    "tableRDD.rdd.filter(lambda row: row.year == 2000).count()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import HiveContext, Row\n",
    "hiveCtx = HiveContext(sc)\n",
    "\n",
    "tablesRDD = hiveCtx.table(\"hive_names_patrick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tablesRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(tablesRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(tablesRDD.rdd\n",
    " .filter(lambda row: row.year == 1880)\n",
    " .filter(lambda row: row.name.startswith('M'))\n",
    " .map(lambda row: row.name)\n",
    " .distinct()\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 3.1.13 Using `\"CREATE TABLE hive_births_YOURNAME AS SELECT ...\"`, create a table in Hive with the number of births per year.  Look up this table in Hue and verify that the data is accessible from outside your Spark session now**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the raw command, just to see the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hiveCtx.sql(\"SELECT year, SUM(births) FROM hive_names_patrick GROUP BY year ORDER BY year ASC\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the DDL statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hiveCtx.sql(\"CREATE TABLE hive_births_patrick AS \"\n",
    "            \"SELECT year, SUM(births) births FROM hive_names_patrick GROUP BY year ORDER BY year ASC\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
